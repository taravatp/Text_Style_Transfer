{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ac7OR0oipo_J"
      ],
      "mount_file_id": "1mqCIdF5BLFwZvntpauk7uGiONj7hIGqm",
      "authorship_tag": "ABX9TyOlLPLzo9Z5s23+mFIj2t4p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/taravatp/Text_Style_Transfer/blob/main/TransRNN_TST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "ZrSCePZcoZ_G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HdR3eYsn450",
        "outputId": "dfeb78a7-58f2-401c-bfce-2b692d110703"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/text_style_transfer\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/text_style_transfer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU hazm\n",
        "!pip install -q transformers==3.1.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGoBWFjXpX3X",
        "outputId": "81507d47-df57-494c-d0b5-cacf18ee4fa0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.7/316.7 KB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 KB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m884.0/884.0 KB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m97.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 KB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "\n",
        "from transformers import BertConfig, BertTokenizer,BertModel\n",
        "from transformers import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "import hazm\n",
        "from hazm import word_tokenize"
      ],
      "metadata": {
        "id": "6aKzT3rvpJiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'device: {device}')\n",
        "\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if not train_on_gpu:\n",
        "    print('CUDA is not available.  Training on CPU ...')\n",
        "else:\n",
        "    print('CUDA is available!  Training on GPU ...')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlojGiczpH7S",
        "outputId": "f8afd4b0-5f85-4145-91c7-9eac11a7b827"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cuda:0\n",
            "CUDA is available!  Training on GPU ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Cleaning"
      ],
      "metadata": {
        "id": "TjAx2wE2ofYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "DATASET_PATH = '/content/drive/MyDrive/text_style_transfer/dataset.xlsx'\n",
        "dataset = pd.read_excel(DATASET_PATH)\n",
        "normalizer = hazm.Normalizer()\n",
        "\n",
        "def cleaning(text):\n",
        "  text = text.strip()\n",
        "  text = normalizer.normalize(text) #normalizing\n",
        "  text = re.sub(r\"([.!?])\", r\" \\1\", text) # inserting a space between words and punctuations\n",
        "  text = re.sub(\"\\s+\", \" \", text) #removing redundant white spaces\n",
        "  return text\n",
        "\n",
        "def truncate(sentence,max_len=20):\n",
        "  if len(word_tokenize(sentence)) < max_len:\n",
        "    return sentence\n",
        "  else:\n",
        "    return None\n",
        "\n",
        "dataset['formalForm'] = dataset['formalForm'].apply(cleaning)\n",
        "dataset['formalForm'] = dataset['formalForm'].apply(truncate)\n",
        "\n",
        "dataset['inFormalForm'] = dataset['inFormalForm'].apply(cleaning)\n",
        "dataset['inFormalForm'] = dataset['inFormalForm'].apply(truncate)\n",
        "\n",
        "dataset = dataset.dropna()\n",
        "dataset = dataset.reset_index()"
      ],
      "metadata": {
        "id": "PLP5_C5ZobgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# saving the cleaned data\n",
        "writePath = '/content/drive/MyDrive/text_style_transfer/CleanedDataset_v2.csv'\n",
        "dataset.to_csv(writePath, encoding='utf-8', index=False)"
      ],
      "metadata": {
        "id": "y7Gh5OCupTnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training embedding models"
      ],
      "metadata": {
        "id": "ac7OR0oipo_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from hazm import word_tokenize\n",
        "import gensim\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "\n",
        "# reading dataset\n",
        "dataset = pd.read_csv(DATASET_PATH)\n",
        "targets = [target for target in dataset.formalForm]\n",
        "targets = [word_tokenize(target) for target in targets]\n",
        "model = Word2Vec(sentences=targets, size=config.hidden_size, window=10, min_count=5, seed=42, workers=5)\n",
        "model.save('targets_embedding.w2v')"
      ],
      "metadata": {
        "id": "rMl-ox6cprSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "9Q_j6Bdv09uV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "  def __init__(self,MODEL_NAME_OR_PATH,config):\n",
        "    super(Encoder,self).__init__()\n",
        "    self.bert = BertModel.from_pretrained(MODEL_NAME_OR_PATH,config=config)\n",
        "\n",
        "  def forward(self,input_ids,attention_mask,token_type_ids):\n",
        "    outputs,pooled_output = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "    # last_hidden_state = outputs.last_hidden_state\n",
        "    return pooled_output # setence representation"
      ],
      "metadata": {
        "id": "pu2uENvs1AQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size,num_layers):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.num_layers = num_layers\n",
        "        #output size is the number of words in the dictionary\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, num_layers)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        # hidden: [1,768] - output: [1,1,768]\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, self.num_layers, self.hidden_size, device=device)"
      ],
      "metadata": {
        "id": "1mVnlUfj1r5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating language style objects - (This will be used for informal sentences)"
      ],
      "metadata": {
        "id": "NczIR6RJs_Tx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "DATASET_PATH = '/content/drive/MyDrive/text_style_transfer/CleanedDataset_v2.csv'\n",
        "dataset = pd.read_csv(DATASET_PATH)"
      ],
      "metadata": {
        "id": "qyOt1VAytCls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LangStyle:\n",
        "  def __init__(self):\n",
        "    self.word2index = {}\n",
        "    self.index2word = {}\n",
        "    self.word2count = {0: \"SOS\", 1: \"EOS\"}\n",
        "    self.n_words = 2\n",
        "\n",
        "  def add_setence_to_lang(self,sentence):\n",
        "    for token in word_tokenize(sentence):\n",
        "      if token not in self.word2index:\n",
        "        self.word2index[token] = self.n_words\n",
        "        self.word2count[token] = 1\n",
        "        self.index2word[self.n_words] = token\n",
        "        self.n_words +=1\n",
        "      else:\n",
        "        self.word2count[token] += 1"
      ],
      "metadata": {
        "id": "UHDPyOtstGf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating DataLoaders"
      ],
      "metadata": {
        "id": "ZuJw8v5ModER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TSTSDATA(Dataset):\n",
        "  def __init__(self, dataset_path, BertTokenizer, config, max_len, formalStyle, flag):\n",
        "\n",
        "    self.dataset = pd.read_csv(dataset_path)\n",
        "    if flag == 'train':\n",
        "      num_samples_train = int(len(self.dataset) * 0.9)\n",
        "      self.dataset = self.dataset.iloc[:num_samples_train]\n",
        "    else:\n",
        "      num_samples_train = int(len(self.dataset) * 0.9)\n",
        "      self.dataset = self.dataset[num_samples_train:]\n",
        "\n",
        "    self.BertTokenizer = BertTokenizer\n",
        "    self.config = config\n",
        "    self.max_len = max_len\n",
        "    self.formalStyle  = formalStyle\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dataset)\n",
        "\n",
        "  def get_encoder_input(self, informal_sentence):\n",
        "\n",
        "    informal_encoding = self.BertTokenizer.encode_plus(\n",
        "    informal_sentence,\n",
        "    add_special_tokens=True,\n",
        "    truncation=True,\n",
        "    max_length=self.max_len,\n",
        "    return_token_type_ids=True,\n",
        "    padding='max_length',\n",
        "    return_attention_mask=True,\n",
        "    return_tensors='pt')\n",
        "\n",
        "\n",
        "    informal_input = {\n",
        "      'informal_sentence': informal_sentence,\n",
        "      'input_ids': informal_encoding['input_ids'].flatten(),\n",
        "      'attention_mask': informal_encoding['attention_mask'].flatten(),\n",
        "      'token_type_ids': informal_encoding['token_type_ids'].flatten()\n",
        "      }\n",
        "\n",
        "    return informal_input\n",
        "\n",
        "  def get_decoder_input(self,formal_sentence):\n",
        "    vector = [self.formalStyle.word2index[word] for word in word_tokenize(formal_sentence)]\n",
        "    vector.append(EOS_token)\n",
        "    vector = torch.tensor(vector, dtype=torch.long)\n",
        "    return vector\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "\n",
        "    inFormalForm = self.dataset['inFormalForm'].iloc[index]\n",
        "    target = self.dataset['formalForm'].iloc[index]\n",
        "\n",
        "    input_encoder = self.get_encoder_input(inFormalForm)\n",
        "    input_decoder = self.get_decoder_input(target)\n",
        "\n",
        "    return (input_encoder, input_decoder, target)"
      ],
      "metadata": {
        "id": "TEpxNjj9oe0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting hyperparameters"
      ],
      "metadata": {
        "id": "-4w9_0tW0RHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 20\n",
        "BATCH_SIZE = 1\n",
        "NUM_EPOCHS = 10\n",
        "NUM_DECODER_LAYERS = 3\n",
        "\n",
        "LEARNING_RATE_ENCODER =  2e-5\n",
        "LEARNING_RATE_DECODER =  0.01\n",
        "TEACHER_FORCE = 0.5\n",
        "\n",
        "DATASET_PATH = '/content/drive/MyDrive/text_style_transfer/CleanedDataset_v2.csv'\n",
        "HAZM_EMBEDDING_PATH = '/content/gdrive/MyDrive/text_style_transfer/targets_embedding.w2v'\n",
        "MODEL_NAME_OR_PATH = 'HooshvareLab/bert-fa-base-uncased'"
      ],
      "metadata": {
        "id": "c6dKvUTQ0Tvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instatntiating required objects"
      ],
      "metadata": {
        "id": "OlzGXglT0b6j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "formalStyle = LangStyle()\n",
        "for index, row in dataset.iterrows():\n",
        "  formalStyle.add_setence_to_lang(row['formalForm'])"
      ],
      "metadata": {
        "id": "JDfBo1ZSzwLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = BertConfig.from_pretrained(MODEL_NAME_OR_PATH)\n",
        "BertTokenizer = BertTokenizer.from_pretrained(MODEL_NAME_OR_PATH)"
      ],
      "metadata": {
        "id": "u5sJ51Uc0Ujn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = TSTSDATA(DATASET_PATH,BertTokenizer,config,MAX_LEN,formalStyle,'train')\n",
        "test_data = TSTSDATA(DATASET_PATH,BertTokenizer,config,MAX_LEN,formalStyle,'test')\n",
        "\n",
        "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE)\n",
        "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "V5uLDketugA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder(MODEL_NAME_OR_PATH, config).to(device)\n",
        "decoder = Decoder(config.hidden_size, formalStyle.n_words, NUM_DECODER_LAYERS).to(device)"
      ],
      "metadata": {
        "id": "M5Pv2crK3ybV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_optimizer = AdamW(encoder.parameters(), lr=LEARNING_RATE_ENCODER, correct_bias=False)\n",
        "decoder_optimizer = torch.optim.SGD(decoder.parameters(),lr=LEARNING_RATE_DECODER)\n",
        "criterion = nn.NLLLoss()"
      ],
      "metadata": {
        "id": "a_mG_GpR00Ur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "6MzcUf0I0fk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_per_epoch = []\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  sumOfLosses = 0\n",
        "  print(f\"starting epoch number : {epoch}\")\n",
        "  for iter,batch in enumerate(test_dataloader):\n",
        "    loss = 0\n",
        "    informal_sentence = batch[0] #[batch_size,num_tokens,1]\n",
        "    formal_sentence  = batch[1].to(device)  #[batch_size,num_tokens,1]\n",
        "\n",
        "    input_ids = informal_sentence['input_ids'].to(device)\n",
        "    attention_mask = informal_sentence['attention_mask'].to(device)\n",
        "    token_type_ids = informal_sentence['token_type_ids'].to(device)\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    sentence_representation = encoder(input_ids=input_ids,attention_mask=attention_mask,token_type_ids=token_type_ids)\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]]).to(device)\n",
        "    decoder_hidden = sentence_representation.view(1,1,-1).to(device)\n",
        "    target_length = formal_sentence.shape[1]\n",
        "\n",
        "    if TEACHER_FORCE > random.random():\n",
        "      for index in range(target_length):\n",
        "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "        loss += criterion(decoder_output, formal_sentence[:,index])\n",
        "        decoder_input = formal_sentence[:,index]\n",
        "    else:\n",
        "      for index in range(target_length):\n",
        "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "        topv, topi = decoder_output.topk(1)\n",
        "        decoder_input = topi.squeeze().detach()\n",
        "        loss += criterion(decoder_output, formal_sentence[:,index])\n",
        "\n",
        "        if decoder_input.item() == EOS_token:\n",
        "          break\n",
        "\n",
        "    sumOfLosses += loss.item() / target_length\n",
        "    loss.backward()\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "  if epoch % 5 == 0:\n",
        "    torch.save(encoder.state_dict(), f\"encoder{epoch}.pth\")\n",
        "    torch.save(decoder.state_dict(), f\"decoder{epoch}.pth\")\n",
        "  loss_per_epoch.append(sumOfLosses/len(test_dataloader))\n",
        "  print(f'end of epoch {epoch} and loss is {sumOfLosses/len(test_dataloader)}')"
      ],
      "metadata": {
        "id": "vgRhPLYZvY8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(encoder.state_dict(), f\"encoder{epoch}.pth\")\n",
        "torch.save(decoder.state_dict(), f\"decoder{epoch}.pth\")"
      ],
      "metadata": {
        "id": "8TkoPUdHFfB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "XQsGAVmgPKVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchtext\n",
        "from torchtext.data.metrics import bleu_score"
      ],
      "metadata": {
        "id": "5Dmz5zXnV6XW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  predicted_sentences = []\n",
        "  bleuScore = 0\n",
        "  for iter,batch in enumerate(test_dataloader):\n",
        "\n",
        "    informal_sentence = batch[0] #[batch_size,num_tokens,1]\n",
        "    formal_sentence  = batch[1].to(device)  #[batch_size,num_tokens,1]\n",
        "    target_sentence = batch[2]\n",
        "\n",
        "    input_ids = informal_sentence['input_ids'].to(device)\n",
        "    attention_mask = informal_sentence['attention_mask'].to(device)\n",
        "    token_type_ids = informal_sentence['token_type_ids'].to(device)\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    sentence_representation = encoder(input_ids=input_ids,attention_mask=attention_mask,token_type_ids=token_type_ids)\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]]).to(device)\n",
        "    decoder_hidden = sentence_representation.view(1,1,-1).to(device)\n",
        "    target_length = formal_sentence.shape[1]\n",
        "\n",
        "\n",
        "    decoded_words = []\n",
        "    for index in range(MAX_LEN):\n",
        "      decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "      topv, topi = decoder_output.data.topk(1)\n",
        "      if topi.item() == EOS_token:\n",
        "        decoded_words.append('<EOS>')\n",
        "        break\n",
        "      else:\n",
        "        decoded_words.append(formalStyle.index2word[topi.item()])\n",
        "        decoder_input = topi.squeeze().detach()\n",
        "\n",
        "    print(target_sentence)\n",
        "    print(decoded_words)\n",
        "    # bleuScore += bleu_score(target_sentence,decoded_words)\n",
        "    predicted_sentences.append(decoded_words)\n",
        "    if iter == 10:\n",
        "      break"
      ],
      "metadata": {
        "id": "cfQjz-0hPL8e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}