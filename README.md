# Text Style Transfer
Text style transfer is an emerging subfield of natural language processing (NLP) that aims to automatically transform the style of a given text while preserving its meaning. The task involves transferring the linguistic features of one style to another, such as changing the tone, formality or politeness level of a task. Text Style Transfer is a challenging problem due to the complex nature of language and the need for preserving semantic coherence. Recent advancaed in deep learning techniques have shown promising results in this area.

## Implemented models:
- **Seq2Seq model with attention mechanism:** Seq2Seq models are a powerfool tool for generating sentences. Comprised of an encoder and a decoder, the implemented model receives the tokens of an informal sentence and generates the corresponding formal tokens. But that's not all - with the implementation of the attention mechanism, the model can leverage the hiiden states outputted at different time-steps by the encoder guide the decoder in generating even more accurate tokens. You can find the implementations of this model at [This](Seq2Seq_TST.ipynb) file.
- **Transformer and RNNs:** This model outperforms the prior seq2seq model. The encoder of this model is powered by a pre-trained large language model, parsBert, which has been specifically trained on persian texts. This means that this model has  a deep understanding of the language and does not learn everything from scratch. Plus, with self attention layers, we can obtain superior representations of input text. This also accelerates both training and inference phases. The decode is similar to the seq2seq model, utilizing recurent layers for optimal performance. You can find the implementations of this model at [This](TransRNN_TST.ipynb) file.
